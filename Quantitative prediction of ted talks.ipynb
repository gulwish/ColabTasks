{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gulwish/ColabTasks/blob/main/Quantitative%20prediction%20of%20ted%20talks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28LtXddxqIm_"
      },
      "source": [
        "# Predicting TED Talks Views with ML Models\n",
        "In this notebook I've done some simple feature engineering on the TED Talks dataset and I've built machine learning models (Random Forest, XGBRegressor, ExtraTreesRegressor, and LGBMRegressor) and optimised their hyperparameters to predict the number of TED Talks views. I've made use of the following kernels to create this notebook:\n",
        "* https://www.kaggle.com/rounakbanik/ted-data-analysis\n",
        "* https://www.kaggle.com/holfyuen/what-makes-a-popular-ted-talk\n",
        "* https://www.kaggle.com/tristanmoser/predicting-a-powerful-idea-a-ted-talk-analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "At_MbdFrqInU",
        "outputId": "fac2b989-96d2-43a9-80be-a139ead25a91"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-201a9eb967ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/ColabNotebooks/TEDTalkViewsPrediction/ted_main.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/ColabNotebooks/TEDTalkViewsPrediction/ted_main.csv'"
          ]
        }
      ],
      "source": [
        "# input data files are available in the \"../input/\" directory\n",
        "# import os\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "import os \n",
        "os.environ['KAGGLE_CONFIG_DIR']='/content'\n",
        "\n",
        "# load libraries\n",
        "import time\n",
        "import warnings\n",
        "import random\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn import metrics\n",
        "from mlxtend.regressor import StackingCVRegressor\n",
        "\n",
        "# ignore warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# load data\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/ColabNotebooks/TEDTalkViewsPrediction/ted_main.csv\")\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RhsLE9_qIni"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "R7gSUMqlslAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM2GSs4dqIno"
      },
      "source": [
        "# TED Talks Data Analysis\n",
        "## Cleaning up the data\n",
        "Various datasets frequently have missing values, so I start off by checking whether the TED Talks dataset has any. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCemah7dqInq"
      },
      "outputs": [],
      "source": [
        "pd.isnull(data).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIjdEwFJqInu"
      },
      "source": [
        "There are only 6 null values in the **speaker_occupation** feature, I will fill in those missing values with a default 'Other' value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7eo1zeFqIn1"
      },
      "outputs": [],
      "source": [
        "for index, row in data.iterrows():\n",
        "    if pd.isnull(row['speaker_occupation']):\n",
        "        data['speaker_occupation'][index] = 'Other'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiaClj0iqIn3"
      },
      "source": [
        "## related_talks\n",
        "Here I print out the **related_talks** feature so I check what it looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1-lf5onqIn6"
      },
      "outputs": [],
      "source": [
        "data['related_talks'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0hG5gAtqIn8"
      },
      "source": [
        "After that I split the string by its commas and then by the semi-column to get the middle element, which is the views of all related talks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRPM2-PDqIn-"
      },
      "outputs": [],
      "source": [
        "data['related_views'] = 0\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "    vids = row['related_talks'].split(',')\n",
        "    counter = 0\n",
        "    total = 0\n",
        "    for views in vids:\n",
        "        if 'viewed_count' in views:\n",
        "            view = views.split(':')\n",
        "            # get rid of brackets and spaces\n",
        "            view[1] = view[1].replace(\"]\", \"\")\n",
        "            view[1] = view[1].replace(\" \", \"\")\n",
        "            view[1] = view[1].replace(\"}\", \"\")\n",
        "            total+=int(view[1])\n",
        "            counter+=1\n",
        "    data['related_views'][index] = total/counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-C8ddQTqIoA"
      },
      "source": [
        "## published_date, filmed_date\n",
        "From these two features I extract day of the week, month, and year."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOTPQy0pqIoB"
      },
      "outputs": [],
      "source": [
        "data['published_date'] = data['published_date'].apply(lambda x: datetime.date.fromtimestamp(int(x)))\n",
        "data['day'] = data['published_date'].apply(lambda x: x.weekday())\n",
        "data['month'] = data['published_date'].apply(lambda x: x.month)\n",
        "data['year'] = data['published_date'].apply(lambda x: x.year)\n",
        "data['film_date'] = data['film_date'].apply(lambda x: datetime.date.fromtimestamp(int(x)))\n",
        "data['day_film'] = data['film_date'].apply(lambda x: x.weekday())\n",
        "data['month_film'] = data['film_date'].apply(lambda x: x.month)\n",
        "data['year_film'] = data['film_date'].apply(lambda x: x.year)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHUyGJ46qIoD"
      },
      "source": [
        "Here I categorise the data which is preferable over using numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbNw-6HcqIoE"
      },
      "outputs": [],
      "source": [
        "to_cat = {\"day\":   {0: \"Monday\", 1: \"Tuesday\", 2: \"Wednesday\", 3: \"Thurday\", 4: \"Friday\", 5: \"Saturday\",\n",
        "                    6: \"Sunday\" },\n",
        "          \"month\": {1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\", 5: \"May\", 6: \"June\", 7: \"July\", 8: \"August\", \n",
        "                    9: \"September\", 10: \"October\", 11: \"November\", 12: \"December\"},\n",
        "          \"year\":  {2006: \"2006\", 2007: \"2007\", 2008: \"2008\", 2009: \"2009\", 2010: \"2010\", 2011: \"2011\", 2012: \"2012\", \n",
        "                    2013: \"2013\", 2014: \"2014\", 2015: \"2015\", 2016: \"2016\", 2017: \"2017\"},\n",
        "          \"day_film\":   {0: \"Monday\", 1: \"Tuesday\", 2: \"Wednesday\", 3: \"Thurday\", 4: \"Friday\", 5: \"Saturday\",\n",
        "                    6: \"Sunday\" },\n",
        "          \"month_film\": {1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\", 5: \"May\", 6: \"June\", 7: \"July\", 8: \"August\", \n",
        "                    9: \"September\", 10: \"October\", 11: \"November\", 12: \"December\"},\n",
        "          \"year_film\":  {2006: \"2006\", 2007: \"2007\", 2008: \"2008\", 2009: \"2009\", 2010: \"2010\", 2011: \"2011\", 2012: \"2012\", \n",
        "                    2013: \"2013\", 2014: \"2014\", 2015: \"2015\", 2016: \"2016\", 2017: \"2017\"}}\n",
        "\n",
        "data.replace(to_cat, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4Dao7mOqIoG"
      },
      "source": [
        "## event\n",
        "I check the number of unique event names then list all of them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mToXQx-XqIoH"
      },
      "outputs": [],
      "source": [
        "print('Number of unique events: ',data['event'].unique().shape[0])\n",
        "data['event'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDePSJ4CqIoJ"
      },
      "source": [
        "The dataset has 355 unique event names but from the looks of it, lots of these names can be categorised together as they are quite similar. I break down the event names in the following 11 categories (each consisting of at least 5 samples)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQBuhN6AqIoK"
      },
      "outputs": [],
      "source": [
        "# initialise all values as 'Other' to assign this category\n",
        "# to all entries that don't fit into the chosen categories\n",
        "data['event_category'] = 'Other'\n",
        "\n",
        "for i in range(len(data)):\n",
        "    if data['event'][i][0:5]=='TED20':\n",
        "        data['event_category'][i] = 'TED2000s'\n",
        "    elif data['event'][i][0:5]=='TED19':\n",
        "        data['event_category'][i] = 'TED1900s'\n",
        "    elif data['event'][i][0:4]=='TEDx':\n",
        "        data['event_category'][i] = \"TEDx\"\n",
        "    elif data['event'][i][0:7]=='TED@BCG':\n",
        "        data['event_category'][i] = 'TED@BCG'\n",
        "    elif data['event'][i][0:4]=='TED@':\n",
        "        data['event_category'][i] = \"TED@\"\n",
        "    elif data['event'][i][0:8]=='TEDSalon':\n",
        "        data['event_category'][i] = \"TEDSalon\"\n",
        "    elif data['event'][i][0:9]=='TEDGlobal':\n",
        "        data['event_category'][i] = 'TEDGlobal'\n",
        "    elif data['event'][i][0:8]=='TEDWomen':\n",
        "        data['event_category'][i] = 'TEDWomen'\n",
        "    elif data['event'][i][0:6]=='TEDMED':\n",
        "        data['event_category'][i] = 'TEDMED'\n",
        "    elif data['event'][i][0:3]=='TED':\n",
        "        data['event_category'][i] = 'TEDOther'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-o4GqIuqIoL"
      },
      "source": [
        "I check whether each categoies can be found in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2ynl1n2qIoN"
      },
      "outputs": [],
      "source": [
        "data['event_category'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(1)\n"
      ],
      "metadata": {
        "id": "qqrPYYIH4vwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e4RBcX-p5B2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tags\n",
        "Using KeyedVectors to encode and categorise the information from the provided tags."
      ],
      "metadata": {
        "id": "hgzuSwui5Jmo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ptsqLLvM5XC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WykOMg68qIoa"
      },
      "source": [
        "## Final touches on the dataset\n",
        "I take out the views and comments and the rest of the features I won't be using."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VdIiksMrxv3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCJt3ofMqIob"
      },
      "outputs": [],
      "source": [
        "views = data['views']\n",
        "comments = data['comments']\n",
        "data = data.drop(['comments', 'description', 'event', 'film_date', 'main_speaker', 'name', 'published_date', 'ratings', \n",
        "           'related_talks', 'tags', 'title', 'url', 'views', 'speaker_occupation', 'Tags'], 1)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data is datframe\n",
        "#csv_data = data.to_csv() \n",
        "\n",
        "\n",
        "data.to_csv(r'C:\\Users\\mahad\\Documents\\cleaned_dataframe.csv', index=False, header=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "2qNORsC9zk8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns.tolist())"
      ],
      "metadata": {
        "id": "xq_6-cBSxw1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FVUZqFCqIod"
      },
      "source": [
        "I apply **One-Hot-Encoding** on the categorical attributes and get the data ready for training machine learning models. Then I print out the dimensions of the final dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KR7-SQkuqIoe"
      },
      "outputs": [],
      "source": [
        "# data2 = data.filter(like='_tag', axis=1)\n",
        "# data = data.drop(data2.columns, 1)\n",
        "# data.head()\n",
        "data_final = pd.get_dummies(data)\n",
        "data_final.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGVu6U9IqIof"
      },
      "source": [
        "# Machine Learning\n",
        "## Random Forest\n",
        "First model I will be testing with is **Random Forest** as this is the one I'm most comfortable with. I will then explore some other ML models, optimise the hyperparameters of each model, and combine those into an ensebmle model.\n",
        "\n",
        "I split the dataset in training (90%) and test (10%) sets. The test data will be later used to validate the ML models on unseen data. I start off with a **Random Forest** as it's quite a powerful model that can be used as a baseline. I will use **Mean Absolute Error (MAE)** to measure the error as it will give us a more intuitive understanding of how accurate the model is. Additionally, using **Mean Squared Error (MSE)** to predict target variables with large values (such as the TED Talks views I'm working with) can lead to problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyS-CmYlqIol"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data_final, views, test_size=0.1, random_state=121212)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqV9QMr2qIom"
      },
      "source": [
        "The baseline Random Forest model seems to have overfitted. Together with the **MAE** of the model I also print the **mean** and **std** of the target variables. Judging by the high variance of the data, it's safe to conclude that the model is performing reasonably well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5PZNFm2qIon"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestRegressor(criterion='mae',max_depth=15, max_features=45, n_estimators=500, min_samples_leaf=2, min_samples_split=2,\n",
        "                           random_state=2019)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_train)\n",
        "y_test_pred = rf.predict(X_test)\n",
        "print('Training MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_train, y_pred)))\n",
        "print('Test MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_test, y_test_pred)))\n",
        "print('Views mean: {:0.2f}'.format(views.mean()))\n",
        "print('Views std: {:0.2f}'.format(views.std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "334qXlo8qIoo"
      },
      "source": [
        "I plot the feature importances that are derived from the model that was just trained. I will use this in the future to exclude the unimportant features with the hope of boosting performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P57QCWbqIoo"
      },
      "outputs": [],
      "source": [
        "importances = pd.DataFrame({'Features': X_train.columns, \n",
        "                                'Importances': rf.feature_importances_})\n",
        "    \n",
        "importances.sort_values(by=['Importances'], axis='index', ascending=False, inplace=True)\n",
        "fig = plt.figure(figsize=(14, 4))\n",
        "sns.barplot(x='Features', y='Importances', data=importances)\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vELVYA93qIop"
      },
      "source": [
        "## XGBRegressor\n",
        "Good accuracy, the model tends to overfit quite easily when **n_estimators > 20**. Maybe it's worth exploring wether we can use a higher **n_estimators** value while using the other hyperparameters to reguralise the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCHzKWg2qIoq"
      },
      "outputs": [],
      "source": [
        "xgbr = xgb.XGBRegressor(criterion='mae', earning_rate=0.1, max_depth=10, subsample=0.5, n_estimators=20, min_child_weight=2, random_state=2019)\n",
        "xgbr.fit(X_train, y_train)\n",
        "y_pred = xgbr.predict(X_train)\n",
        "y_test_pred = xgbr.predict(X_test)\n",
        "print('Training MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_train, y_pred)))\n",
        "print('Test MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_test, y_test_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfjAr6zgqIor"
      },
      "source": [
        "## ExtraTreesRegressor\n",
        "ExtraTreesRegressor yields the best accuracy, maybe can reguralise the model better as the gap between Training and Test MAE is quite big?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNbOT_zXqIos"
      },
      "outputs": [],
      "source": [
        "et = ExtraTreesRegressor(criterion='mae', max_depth=30, n_estimators=1000, random_state=2019, min_samples_leaf=2, min_samples_split=6)\n",
        "et.fit(X_train, y_train)\n",
        "y_pred = et.predict(X_train)\n",
        "y_test_pred = et.predict(X_test)\n",
        "print('Training MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_train, y_pred)))\n",
        "print('Test MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_test, y_test_pred)))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "V4Dao7mOqIoG",
        "H8biI15oqIoP",
        "WykOMg68qIoa",
        "vELVYA93qIop",
        "bfjAr6zgqIor",
        "E9Mo_pzGqIot",
        "-xLUN2N9qIov"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}